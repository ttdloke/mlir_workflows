{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXFBlGSn8OOx",
        "outputId": "9564b4c1-ad72-41ec-d5af-95402209f472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: jax 0.4.26\n",
            "Uninstalling jax-0.4.26:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/jax-0.4.26.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/jax/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled jax-0.4.26\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall jax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbf6vWo18uSi",
        "outputId": "2781bfbe-a7f3-46e8-b230-4a767a98b4c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax\n",
            "  Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxlib<=0.4.30,>=0.4.27 (from jax)\n",
            "  Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax) (1.11.4)\n",
            "Installing collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n",
            "Successfully installed jax-0.4.30 jaxlib-0.4.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random"
      ],
      "metadata": {
        "id": "zdGKiwIU8v0W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A helper function to randomly initialize weights and biases\n",
        "# for a dense neural network layer\n",
        "def random_layer_params(m, n, key, scale=1e-2):\n",
        "  w_key, b_key = random.split(key)\n",
        "  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
        "\n",
        "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
        "def init_network_params(sizes, key):\n",
        "  keys = random.split(key, len(sizes))\n",
        "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
        "\n",
        "layer_sizes = [784, 512, 512, 10]\n",
        "step_size = 0.01\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "n_targets = 10\n",
        "params = init_network_params(layer_sizes, random.key(0))"
      ],
      "metadata": {
        "id": "9tR9SIDh8zKE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.scipy.special import logsumexp\n",
        "\n",
        "def relu(x):\n",
        "  return jnp.maximum(0, x)\n",
        "\n",
        "def predict(params, image):\n",
        "  # per-example predictions\n",
        "  activations = image\n",
        "  for w, b in params[:-1]:\n",
        "    outputs = jnp.dot(w, activations) + b\n",
        "    activations = relu(outputs)\n",
        "\n",
        "  final_w, final_b = params[-1]\n",
        "  logits = jnp.dot(final_w, activations) + final_b\n",
        "  return logits - logsumexp(logits)"
      ],
      "metadata": {
        "id": "1y9ASODs8zYu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This works on single examples\n",
        "random_flattened_image = random.normal(random.key(1), (28 * 28,))\n",
        "preds = predict(params, random_flattened_image)\n",
        "print(preds.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeMVwfN7817j",
        "outputId": "b774ecb1-b369-416e-bfdb-a13d4e7768ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doesn't work with a batch\n",
        "random_flattened_images = random.normal(random.key(1), (10, 28 * 28))\n",
        "try:\n",
        "  preds = predict(params, random_flattened_images)\n",
        "except TypeError:\n",
        "  print('Invalid shapes!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAjw_2er9D5s",
        "outputId": "338a730e-9905-4528-cb9e-b680fa17242c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid shapes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's upgrade it to handle batches using `vmap`\n",
        "\n",
        "# Make a batched version of the `predict` function\n",
        "batched_predict = vmap(predict, in_axes=(None, 0))\n",
        "\n",
        "# `batched_predict` has the same call signature as `predict`\n",
        "batched_preds = batched_predict(params, random_flattened_images)\n",
        "print(batched_preds.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh7MH01i9Jbz",
        "outputId": "78456e7d-af86-4ee6-b1cc-e060db76da2e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(x, k, dtype=jnp.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
        "\n",
        "def accuracy(params, images, targets):\n",
        "  target_class = jnp.argmax(targets, axis=1)\n",
        "  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
        "  return jnp.mean(predicted_class == target_class)\n",
        "\n",
        "def loss(params, images, targets):\n",
        "  preds = batched_predict(params, images)\n",
        "  return -jnp.mean(preds * targets)\n",
        "\n",
        "# @jit\n",
        "def update(params, x, y):\n",
        "  grads = grad(loss)(params, x, y)\n",
        "  return [(w - step_size * dw, b - step_size * db)\n",
        "          for (w, b), (dw, db) in zip(params, grads)]"
      ],
      "metadata": {
        "id": "9kjFdkWS9LSk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Ensure TF does not see GPU and grab all GPU memory.\n",
        "tf.config.set_visible_devices([], device_type='GPU')\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "data_dir = '/tmp/tfds'\n",
        "\n",
        "# Fetch full datasets for evaluation\n",
        "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
        "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
        "mnist_data = tfds.as_numpy(mnist_data)\n",
        "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
        "num_labels = info.features['label'].num_classes\n",
        "h, w, c = info.features['image'].shape\n",
        "num_pixels = h * w * c\n",
        "\n",
        "# Full train set\n",
        "train_images, train_labels = train_data['image'], train_data['label']\n",
        "train_images = jnp.reshape(train_images, (len(train_images), num_pixels))\n",
        "train_labels = one_hot(train_labels, num_labels)\n",
        "\n",
        "# Full test set\n",
        "test_images, test_labels = test_data['image'], test_data['label']\n",
        "test_images = jnp.reshape(test_images, (len(test_images), num_pixels))\n",
        "test_labels = one_hot(test_labels, num_labels)"
      ],
      "metadata": {
        "id": "QUobVULQ9NMT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train:', train_images.shape, train_labels.shape)\n",
        "print('Test:', test_images.shape, test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "robTDhpK9QkB",
        "outputId": "3d472c43-e0a0-4af8-9011-629434055507"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (60000, 784) (60000, 10)\n",
            "Test: (10000, 784) (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def get_train_batches():\n",
        "  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
        "  ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n",
        "  # You can build up an arbitrary tf.data input pipeline\n",
        "  ds = ds.batch(batch_size).prefetch(1)\n",
        "  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n",
        "  return tfds.as_numpy(ds)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "  for x, y in get_train_batches():\n",
        "    # print(type(x))\n",
        "    # print(type(y))\n",
        "    # print(type(params))\n",
        "    x = jnp.reshape(x, (len(x), num_pixels))\n",
        "    y = one_hot(y, num_labels)\n",
        "    params = update(params, x, y)\n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        "  train_acc = accuracy(params, train_images, train_labels)\n",
        "  test_acc = accuracy(params, test_images, test_labels)\n",
        "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "  print(\"Training set accuracy {}\".format(train_acc))\n",
        "  print(\"Test set accuracy {}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLUqDQP89Z3t",
        "outputId": "f4de1c04-5ec7-4656-8830-ee0228e8ed65"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 in 24.42 sec\n",
            "Training set accuracy 0.9948000311851501\n",
            "Test set accuracy 0.9777999520301819\n",
            "Epoch 1 in 23.85 sec\n",
            "Training set accuracy 0.9951666593551636\n",
            "Test set accuracy 0.9781999588012695\n",
            "Epoch 2 in 23.87 sec\n",
            "Training set accuracy 0.9955000281333923\n",
            "Test set accuracy 0.9781000018119812\n",
            "Epoch 3 in 24.11 sec\n",
            "Training set accuracy 0.99590003490448\n",
            "Test set accuracy 0.9781999588012695\n",
            "Epoch 4 in 23.66 sec\n",
            "Training set accuracy 0.9962000250816345\n",
            "Test set accuracy 0.9781999588012695\n",
            "Epoch 5 in 24.29 sec\n",
            "Training set accuracy 0.9965833425521851\n",
            "Test set accuracy 0.9782999753952026\n",
            "Epoch 6 in 23.96 sec\n",
            "Training set accuracy 0.9968667030334473\n",
            "Test set accuracy 0.9785999655723572\n",
            "Epoch 7 in 24.18 sec\n",
            "Training set accuracy 0.9971333742141724\n",
            "Test set accuracy 0.9787999987602234\n",
            "Epoch 8 in 41.09 sec\n",
            "Training set accuracy 0.9973166584968567\n",
            "Test set accuracy 0.9788999557495117\n",
            "Epoch 9 in 23.83 sec\n",
            "Training set accuracy 0.9975500106811523\n",
            "Test set accuracy 0.9788999557495117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax._src.interpreters import mlir as jax_mlir\n",
        "from jax._src.lib.mlir import ir\n",
        "\n",
        "# Returns prettyprint of StableHLO module without large constants\n",
        "def get_stablehlo_asm(module_str):\n",
        "  with jax_mlir.make_ir_context():\n",
        "    stablehlo_module = ir.Module.parse(module_str, context=jax_mlir.make_ir_context())\n",
        "    return stablehlo_module.operation.get_asm(large_elements_limit=20)\n",
        "\n",
        "# Disable logging for better tutorial rendering\n",
        "import logging\n",
        "logging.disable(logging.WARNING)"
      ],
      "metadata": {
        "id": "qXPAhgmV_RW9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import export\n",
        "import numpy as np\n",
        "\n",
        "def plus(x,y,z):\n",
        "  a = x + y\n",
        "  return a + z\n",
        "\n",
        "exp = export.export(jax.jit(plus))(\n",
        "   jax.ShapeDtypeStruct((), np.float32), jax.ShapeDtypeStruct((), np.float32), jax.ShapeDtypeStruct((), np.float32)).mlir_module()\n",
        "print(get_stablehlo_asm(exp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIjBCfzY_Mdo",
        "outputId": "98781376-c436-4163-e2a6-1432087e50e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_plus attributes {jax.uses_shape_polymorphism = false, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<f32> {mhlo.layout_mode = \"default\"}, %arg1: tensor<f32> {mhlo.layout_mode = \"default\"}, %arg2: tensor<f32> {mhlo.layout_mode = \"default\"}) -> (tensor<f32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n",
            "    %0 = stablehlo.add %arg0, %arg1 : tensor<f32>\n",
            "    %1 = stablehlo.add %0, %arg2 : tensor<f32>\n",
            "    return %1 : tensor<f32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exp = export.export(jax.jit(update))\n",
        "# print(l, m)\n",
        "lowered = jax.jit(update).lower(params, x, y)\n",
        "print(lowered.as_text())\n",
        "# (jax.ShapeDtypeStruct((), np.ndarray), jax.ShapeDtypeStruct((), np.ndarray), jax.ShapeDtypeStruct((), np.ndarray)).mlir_module()\n",
        "# print(get_stablehlo_asm(exp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fXiWkhV_XJW",
        "outputId": "1bb637d4-b7ba-44ae-a7fc-11dc5e8f824e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_update attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<512x784xf32> {mhlo.layout_mode = \"default\"}, %arg1: tensor<512xf32> {mhlo.layout_mode = \"default\"}, %arg2: tensor<512x512xf32> {mhlo.layout_mode = \"default\"}, %arg3: tensor<512xf32> {mhlo.layout_mode = \"default\"}, %arg4: tensor<10x512xf32> {mhlo.layout_mode = \"default\"}, %arg5: tensor<10xf32> {mhlo.layout_mode = \"default\"}, %arg6: tensor<96x784xui8> {mhlo.layout_mode = \"default\"}, %arg7: tensor<96x10xf32> {mhlo.layout_mode = \"default\"}) -> (tensor<512x784xf32> {jax.result_info = \"[0][0]\", mhlo.layout_mode = \"default\"}, tensor<512xf32> {jax.result_info = \"[0][1]\", mhlo.layout_mode = \"default\"}, tensor<512x512xf32> {jax.result_info = \"[1][0]\", mhlo.layout_mode = \"default\"}, tensor<512xf32> {jax.result_info = \"[1][1]\", mhlo.layout_mode = \"default\"}, tensor<10x512xf32> {jax.result_info = \"[2][0]\", mhlo.layout_mode = \"default\"}, tensor<10xf32> {jax.result_info = \"[2][1]\", mhlo.layout_mode = \"default\"}) {\n",
            "    %0 = stablehlo.convert %arg0 : tensor<512x784xf32>\n",
            "    %1 = stablehlo.convert %arg6 : (tensor<96x784xui8>) -> tensor<96x784xf32>\n",
            "    %2 = stablehlo.dot_general %0, %1, contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<512x784xf32>, tensor<96x784xf32>) -> tensor<512x96xf32>\n",
            "    %3 = stablehlo.transpose %2, dims = [1, 0] : (tensor<512x96xf32>) -> tensor<96x512xf32>\n",
            "    %4 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<512xf32>) -> tensor<1x512xf32>\n",
            "    %5 = stablehlo.broadcast_in_dim %4, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<96x512xf32>\n",
            "    %6 = stablehlo.add %3, %5 : tensor<96x512xf32>\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %7 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %8 = stablehlo.maximum %7, %6 : tensor<96x512xf32>\n",
            "    %9 = stablehlo.compare  EQ, %6, %8,  FLOAT : (tensor<96x512xf32>, tensor<96x512xf32>) -> tensor<96x512xi1>\n",
            "    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n",
            "    %10 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %11 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %12 = stablehlo.select %9, %10, %11 : tensor<96x512xi1>, tensor<96x512xf32>\n",
            "    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %13 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %14 = stablehlo.compare  EQ, %13, %8,  FLOAT : (tensor<96x512xf32>, tensor<96x512xf32>) -> tensor<96x512xi1>\n",
            "    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n",
            "    %15 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %cst_4 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n",
            "    %16 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %17 = stablehlo.select %14, %15, %16 : tensor<96x512xi1>, tensor<96x512xf32>\n",
            "    %18 = stablehlo.divide %12, %17 : tensor<96x512xf32>\n",
            "    %19 = stablehlo.dot_general %arg2, %8, contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<512x512xf32>, tensor<96x512xf32>) -> tensor<512x96xf32>\n",
            "    %20 = stablehlo.transpose %19, dims = [1, 0] : (tensor<512x96xf32>) -> tensor<96x512xf32>\n",
            "    %21 = stablehlo.broadcast_in_dim %arg3, dims = [1] : (tensor<512xf32>) -> tensor<1x512xf32>\n",
            "    %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<96x512xf32>\n",
            "    %23 = stablehlo.add %20, %22 : tensor<96x512xf32>\n",
            "    %cst_5 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %24 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %25 = stablehlo.maximum %24, %23 : tensor<96x512xf32>\n",
            "    %26 = stablehlo.compare  EQ, %23, %25,  FLOAT : (tensor<96x512xf32>, tensor<96x512xf32>) -> tensor<96x512xi1>\n",
            "    %cst_6 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n",
            "    %27 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %28 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %29 = stablehlo.select %26, %27, %28 : tensor<96x512xi1>, tensor<96x512xf32>\n",
            "    %cst_8 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %30 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %31 = stablehlo.compare  EQ, %30, %25,  FLOAT : (tensor<96x512xf32>, tensor<96x512xf32>) -> tensor<96x512xi1>\n",
            "    %cst_9 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n",
            "    %32 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %cst_10 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n",
            "    %33 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<f32>) -> tensor<96x512xf32>\n",
            "    %34 = stablehlo.select %31, %32, %33 : tensor<96x512xi1>, tensor<96x512xf32>\n",
            "    %35 = stablehlo.divide %29, %34 : tensor<96x512xf32>\n",
            "    %36 = stablehlo.dot_general %arg4, %25, contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<10x512xf32>, tensor<96x512xf32>) -> tensor<10x96xf32>\n",
            "    %37 = stablehlo.transpose %36, dims = [1, 0] : (tensor<10x96xf32>) -> tensor<96x10xf32>\n",
            "    %38 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n",
            "    %39 = stablehlo.broadcast_in_dim %38, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<96x10xf32>\n",
            "    %40 = stablehlo.add %37, %39 : tensor<96x10xf32>\n",
            "    %cst_11 = stablehlo.constant dense<0xFF800000> : tensor<f32>\n",
            "    %41 = stablehlo.reduce(%40 init: %cst_11) applies stablehlo.maximum across dimensions = [1] : (tensor<96x10xf32>, tensor<f32>) -> tensor<96xf32>\n",
            "    %cst_12 = stablehlo.constant dense<0xFF800000> : tensor<f32>\n",
            "    %42 = stablehlo.broadcast_in_dim %cst_12, dims = [] : (tensor<f32>) -> tensor<96xf32>\n",
            "    %43 = stablehlo.maximum %42, %41 : tensor<96xf32>\n",
            "    %44 = stablehlo.is_finite %43 : (tensor<96xf32>) -> tensor<96xi1>\n",
            "    %cst_13 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %45 = stablehlo.broadcast_in_dim %cst_13, dims = [] : (tensor<f32>) -> tensor<96xf32>\n",
            "    %46 = stablehlo.select %44, %43, %45 : tensor<96xi1>, tensor<96xf32>\n",
            "    %47 = stablehlo.broadcast_in_dim %46, dims = [0] : (tensor<96xf32>) -> tensor<96x1xf32>\n",
            "    %48 = stablehlo.broadcast_in_dim %47, dims = [0, 1] : (tensor<96x1xf32>) -> tensor<96x10xf32>\n",
            "    %49 = stablehlo.subtract %40, %48 : tensor<96x10xf32>\n",
            "    %50 = stablehlo.exponential %49 : tensor<96x10xf32>\n",
            "    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %51 = stablehlo.reduce(%50 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<96x10xf32>, tensor<f32>) -> tensor<96xf32>\n",
            "    %52 = stablehlo.abs %51 : tensor<96xf32>\n",
            "    %cst_15 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %53 = stablehlo.broadcast_in_dim %cst_15, dims = [] : (tensor<f32>) -> tensor<96xf32>\n",
            "    %54 = stablehlo.compare  GE, %51, %53,  FLOAT : (tensor<96xf32>, tensor<96xf32>) -> tensor<96xi1>\n",
            "    %cst_16 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n",
            "    %55 = stablehlo.negate %cst_16 : tensor<f32>\n",
            "    %cst_17 = stablehlo.constant dense<9.600000e+02> : tensor<f32>\n",
            "    %56 = stablehlo.divide %55, %cst_17 : tensor<f32>\n",
            "    %57 = stablehlo.broadcast_in_dim %56, dims = [] : (tensor<f32>) -> tensor<96x10xf32>\n",
            "    %58 = stablehlo.multiply %57, %arg7 : tensor<96x10xf32>\n",
            "    %59 = stablehlo.negate %58 : tensor<96x10xf32>\n",
            "    %cst_18 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %60 = stablehlo.reduce(%59 init: %cst_18) applies stablehlo.add across dimensions = [1] : (tensor<96x10xf32>, tensor<f32>) -> tensor<96xf32>\n",
            "    %61 = stablehlo.reshape %60 : (tensor<96xf32>) -> tensor<96x1xf32>\n",
            "    %cst_19 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %62 = stablehlo.reduce(%61 init: %cst_19) applies stablehlo.add across dimensions = [1] : (tensor<96x1xf32>, tensor<f32>) -> tensor<96xf32>\n",
            "    %63 = stablehlo.divide %62, %52 : tensor<96xf32>\n",
            "    %cst_20 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %64 = stablehlo.broadcast_in_dim %cst_20, dims = [] : (tensor<f32>) -> tensor<96xf32>\n",
            "    %65 = stablehlo.select %54, %64, %63 : tensor<96xi1>, tensor<96xf32>\n",
            "    %66 = stablehlo.select %54, %63, %64 : tensor<96xi1>, tensor<96xf32>\n",
            "    %67 = stablehlo.negate %65 : tensor<96xf32>\n",
            "    %68 = stablehlo.add %66, %67 : tensor<96xf32>\n",
            "    %69 = stablehlo.broadcast_in_dim %68, dims = [0] : (tensor<96xf32>) -> tensor<96x10xf32>\n",
            "    %70 = stablehlo.multiply %69, %50 : tensor<96x10xf32>\n",
            "    %71 = stablehlo.add %58, %70 : tensor<96x10xf32>\n",
            "    %cst_21 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %72 = stablehlo.reduce(%71 init: %cst_21) applies stablehlo.add across dimensions = [0] : (tensor<96x10xf32>, tensor<f32>) -> tensor<10xf32>\n",
            "    %73 = stablehlo.reshape %72 : (tensor<10xf32>) -> tensor<1x10xf32>\n",
            "    %cst_22 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %74 = stablehlo.reduce(%73 init: %cst_22) applies stablehlo.add across dimensions = [0] : (tensor<1x10xf32>, tensor<f32>) -> tensor<10xf32>\n",
            "    %75 = stablehlo.transpose %71, dims = [1, 0] : (tensor<96x10xf32>) -> tensor<10x96xf32>\n",
            "    %76 = stablehlo.dot_general %75, %arg4, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x96xf32>, tensor<10x512xf32>) -> tensor<96x512xf32>\n",
            "    %77 = stablehlo.dot_general %75, %25, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x96xf32>, tensor<96x512xf32>) -> tensor<10x512xf32>\n",
            "    %78 = stablehlo.multiply %76, %35 : tensor<96x512xf32>\n",
            "    %cst_23 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %79 = stablehlo.reduce(%78 init: %cst_23) applies stablehlo.add across dimensions = [0] : (tensor<96x512xf32>, tensor<f32>) -> tensor<512xf32>\n",
            "    %80 = stablehlo.reshape %79 : (tensor<512xf32>) -> tensor<1x512xf32>\n",
            "    %cst_24 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %81 = stablehlo.reduce(%80 init: %cst_24) applies stablehlo.add across dimensions = [0] : (tensor<1x512xf32>, tensor<f32>) -> tensor<512xf32>\n",
            "    %82 = stablehlo.transpose %78, dims = [1, 0] : (tensor<96x512xf32>) -> tensor<512x96xf32>\n",
            "    %83 = stablehlo.dot_general %82, %arg2, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<512x96xf32>, tensor<512x512xf32>) -> tensor<96x512xf32>\n",
            "    %84 = stablehlo.dot_general %82, %8, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<512x96xf32>, tensor<96x512xf32>) -> tensor<512x512xf32>\n",
            "    %85 = stablehlo.multiply %83, %18 : tensor<96x512xf32>\n",
            "    %cst_25 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %86 = stablehlo.reduce(%85 init: %cst_25) applies stablehlo.add across dimensions = [0] : (tensor<96x512xf32>, tensor<f32>) -> tensor<512xf32>\n",
            "    %87 = stablehlo.reshape %86 : (tensor<512xf32>) -> tensor<1x512xf32>\n",
            "    %cst_26 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %88 = stablehlo.reduce(%87 init: %cst_26) applies stablehlo.add across dimensions = [0] : (tensor<1x512xf32>, tensor<f32>) -> tensor<512xf32>\n",
            "    %89 = stablehlo.transpose %85, dims = [1, 0] : (tensor<96x512xf32>) -> tensor<512x96xf32>\n",
            "    %90 = stablehlo.convert %89 : tensor<512x96xf32>\n",
            "    %91 = stablehlo.convert %arg6 : (tensor<96x784xui8>) -> tensor<96x784xf32>\n",
            "    %92 = stablehlo.dot_general %90, %91, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<512x96xf32>, tensor<96x784xf32>) -> tensor<512x784xf32>\n",
            "    %cst_27 = stablehlo.constant dense<0.00999999977> : tensor<f32>\n",
            "    %93 = stablehlo.broadcast_in_dim %cst_27, dims = [] : (tensor<f32>) -> tensor<512x784xf32>\n",
            "    %94 = stablehlo.multiply %93, %92 : tensor<512x784xf32>\n",
            "    %95 = stablehlo.subtract %arg0, %94 : tensor<512x784xf32>\n",
            "    %cst_28 = stablehlo.constant dense<0.00999999977> : tensor<f32>\n",
            "    %96 = stablehlo.broadcast_in_dim %cst_28, dims = [] : (tensor<f32>) -> tensor<512xf32>\n",
            "    %97 = stablehlo.multiply %96, %88 : tensor<512xf32>\n",
            "    %98 = stablehlo.subtract %arg1, %97 : tensor<512xf32>\n",
            "    %cst_29 = stablehlo.constant dense<0.00999999977> : tensor<f32>\n",
            "    %99 = stablehlo.broadcast_in_dim %cst_29, dims = [] : (tensor<f32>) -> tensor<512x512xf32>\n",
            "    %100 = stablehlo.multiply %99, %84 : tensor<512x512xf32>\n",
            "    %101 = stablehlo.subtract %arg2, %100 : tensor<512x512xf32>\n",
            "    %cst_30 = stablehlo.constant dense<0.00999999977> : tensor<f32>\n",
            "    %102 = stablehlo.broadcast_in_dim %cst_30, dims = [] : (tensor<f32>) -> tensor<512xf32>\n",
            "    %103 = stablehlo.multiply %102, %81 : tensor<512xf32>\n",
            "    %104 = stablehlo.subtract %arg3, %103 : tensor<512xf32>\n",
            "    %cst_31 = stablehlo.constant dense<0.00999999977> : tensor<f32>\n",
            "    %105 = stablehlo.broadcast_in_dim %cst_31, dims = [] : (tensor<f32>) -> tensor<10x512xf32>\n",
            "    %106 = stablehlo.multiply %105, %77 : tensor<10x512xf32>\n",
            "    %107 = stablehlo.subtract %arg4, %106 : tensor<10x512xf32>\n",
            "    %cst_32 = stablehlo.constant dense<0.00999999977> : tensor<f32>\n",
            "    %108 = stablehlo.broadcast_in_dim %cst_32, dims = [] : (tensor<f32>) -> tensor<10xf32>\n",
            "    %109 = stablehlo.multiply %108, %74 : tensor<10xf32>\n",
            "    %110 = stablehlo.subtract %arg5, %109 : tensor<10xf32>\n",
            "    return %95, %98, %101, %104, %107, %110 : tensor<512x784xf32>, tensor<512xf32>, tensor<512x512xf32>, tensor<512xf32>, tensor<10x512xf32>, tensor<10xf32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lowered2 = jax.jit(accuracy).lower(params, test_images, test_labels)\n",
        "print(lowered2.as_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCqrS7_DADZx",
        "outputId": "34a4fba4-1196-43e9-f8b7-f3cfd4516033"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_accuracy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<512x784xf32> {mhlo.layout_mode = \"default\"}, %arg1: tensor<512xf32> {mhlo.layout_mode = \"default\"}, %arg2: tensor<512x512xf32> {mhlo.layout_mode = \"default\"}, %arg3: tensor<512xf32> {mhlo.layout_mode = \"default\"}, %arg4: tensor<10x512xf32> {mhlo.layout_mode = \"default\"}, %arg5: tensor<10xf32> {mhlo.layout_mode = \"default\"}, %arg6: tensor<10000x784xui8> {mhlo.layout_mode = \"default\"}, %arg7: tensor<10000x10xf32> {mhlo.layout_mode = \"default\"}) -> (tensor<f32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n",
            "    %0 = call @argmax(%arg7) : (tensor<10000x10xf32>) -> tensor<10000xi32>\n",
            "    %1 = stablehlo.convert %arg0 : tensor<512x784xf32>\n",
            "    %2 = stablehlo.convert %arg6 : (tensor<10000x784xui8>) -> tensor<10000x784xf32>\n",
            "    %3 = stablehlo.dot_general %1, %2, contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<512x784xf32>, tensor<10000x784xf32>) -> tensor<512x10000xf32>\n",
            "    %4 = stablehlo.transpose %3, dims = [1, 0] : (tensor<512x10000xf32>) -> tensor<10000x512xf32>\n",
            "    %5 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<512xf32>) -> tensor<1x512xf32>\n",
            "    %6 = stablehlo.broadcast_in_dim %5, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<10000x512xf32>\n",
            "    %7 = stablehlo.add %4, %6 : tensor<10000x512xf32>\n",
            "    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %8 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10000x512xf32>\n",
            "    %9 = stablehlo.maximum %8, %7 : tensor<10000x512xf32>\n",
            "    %10 = stablehlo.dot_general %arg2, %9, contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<512x512xf32>, tensor<10000x512xf32>) -> tensor<512x10000xf32>\n",
            "    %11 = stablehlo.transpose %10, dims = [1, 0] : (tensor<512x10000xf32>) -> tensor<10000x512xf32>\n",
            "    %12 = stablehlo.broadcast_in_dim %arg3, dims = [1] : (tensor<512xf32>) -> tensor<1x512xf32>\n",
            "    %13 = stablehlo.broadcast_in_dim %12, dims = [0, 1] : (tensor<1x512xf32>) -> tensor<10000x512xf32>\n",
            "    %14 = stablehlo.add %11, %13 : tensor<10000x512xf32>\n",
            "    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %15 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10000x512xf32>\n",
            "    %16 = stablehlo.maximum %15, %14 : tensor<10000x512xf32>\n",
            "    %17 = stablehlo.dot_general %arg4, %16, contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<10x512xf32>, tensor<10000x512xf32>) -> tensor<10x10000xf32>\n",
            "    %18 = stablehlo.transpose %17, dims = [1, 0] : (tensor<10x10000xf32>) -> tensor<10000x10xf32>\n",
            "    %19 = stablehlo.broadcast_in_dim %arg5, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n",
            "    %20 = stablehlo.broadcast_in_dim %19, dims = [0, 1] : (tensor<1x10xf32>) -> tensor<10000x10xf32>\n",
            "    %21 = stablehlo.add %18, %20 : tensor<10000x10xf32>\n",
            "    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>\n",
            "    %22 = stablehlo.reduce(%21 init: %cst_1) applies stablehlo.maximum across dimensions = [1] : (tensor<10000x10xf32>, tensor<f32>) -> tensor<10000xf32>\n",
            "    %cst_2 = stablehlo.constant dense<0xFF800000> : tensor<f32>\n",
            "    %23 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<10000xf32>\n",
            "    %24 = stablehlo.maximum %23, %22 : tensor<10000xf32>\n",
            "    %25 = stablehlo.is_finite %24 : (tensor<10000xf32>) -> tensor<10000xi1>\n",
            "    %cst_3 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %26 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<10000xf32>\n",
            "    %27 = stablehlo.select %25, %24, %26 : tensor<10000xi1>, tensor<10000xf32>\n",
            "    %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<10000xf32>) -> tensor<10000x1xf32>\n",
            "    %29 = stablehlo.broadcast_in_dim %28, dims = [0, 1] : (tensor<10000x1xf32>) -> tensor<10000x10xf32>\n",
            "    %30 = stablehlo.subtract %21, %29 : tensor<10000x10xf32>\n",
            "    %31 = stablehlo.exponential %30 : tensor<10000x10xf32>\n",
            "    %cst_4 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %32 = stablehlo.reduce(%31 init: %cst_4) applies stablehlo.add across dimensions = [1] : (tensor<10000x10xf32>, tensor<f32>) -> tensor<10000xf32>\n",
            "    %33 = stablehlo.abs %32 : tensor<10000xf32>\n",
            "    %34 = stablehlo.log %33 : tensor<10000xf32>\n",
            "    %35 = stablehlo.add %34, %27 : tensor<10000xf32>\n",
            "    %36 = stablehlo.broadcast_in_dim %35, dims = [0] : (tensor<10000xf32>) -> tensor<10000x1xf32>\n",
            "    %37 = stablehlo.broadcast_in_dim %36, dims = [0, 1] : (tensor<10000x1xf32>) -> tensor<10000x10xf32>\n",
            "    %38 = stablehlo.subtract %21, %37 : tensor<10000x10xf32>\n",
            "    %39 = call @argmax(%38) : (tensor<10000x10xf32>) -> tensor<10000xi32>\n",
            "    %40 = stablehlo.compare  EQ, %39, %0,  SIGNED : (tensor<10000xi32>, tensor<10000xi32>) -> tensor<10000xi1>\n",
            "    %41 = stablehlo.convert %40 : (tensor<10000xi1>) -> tensor<10000xi32>\n",
            "    %42 = stablehlo.convert %41 : (tensor<10000xi32>) -> tensor<10000xf32>\n",
            "    %cst_5 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %43 = stablehlo.reduce(%42 init: %cst_5) applies stablehlo.add across dimensions = [0] : (tensor<10000xf32>, tensor<f32>) -> tensor<f32>\n",
            "    %cst_6 = stablehlo.constant dense<1.000000e+04> : tensor<f32>\n",
            "    %44 = stablehlo.divide %43, %cst_6 : tensor<f32>\n",
            "    return %44 : tensor<f32>\n",
            "  }\n",
            "  func.func private @argmax(%arg0: tensor<10000x10xf32>) -> tensor<10000xi32> {\n",
            "    %0 = stablehlo.iota dim = 1 : tensor<10000x10xi32>\n",
            "    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>\n",
            "    %c = stablehlo.constant dense<0> : tensor<i32>\n",
            "    %1:2 = stablehlo.reduce(%arg0 init: %cst), (%0 init: %c) across dimensions = [1] : (tensor<10000x10xf32>, tensor<10000x10xi32>, tensor<f32>, tensor<i32>) -> (tensor<10000xf32>, tensor<10000xi32>)\n",
            "     reducer(%arg1: tensor<f32>, %arg3: tensor<f32>) (%arg2: tensor<i32>, %arg4: tensor<i32>)  {\n",
            "      %2 = stablehlo.compare  GT, %arg1, %arg3,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n",
            "      %3 = stablehlo.compare  NE, %arg1, %arg1,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n",
            "      %4 = stablehlo.or %2, %3 : tensor<i1>\n",
            "      %5 = stablehlo.compare  EQ, %arg1, %arg3,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n",
            "      %6 = stablehlo.compare  LT, %arg2, %arg4,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n",
            "      %7 = stablehlo.and %5, %6 : tensor<i1>\n",
            "      %8 = stablehlo.or %4, %7 : tensor<i1>\n",
            "      %9 = stablehlo.select %4, %arg1, %arg3 : tensor<i1>, tensor<f32>\n",
            "      %10 = stablehlo.select %8, %arg2, %arg4 : tensor<i1>, tensor<i32>\n",
            "      stablehlo.return %9, %10 : tensor<f32>, tensor<i32>\n",
            "    }\n",
            "    return %1#1 : tensor<10000xi32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BORuT6j_N7LJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}