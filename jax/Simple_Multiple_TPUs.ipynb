{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Need to have a TPU go to edit and then notebook settings and click TPU V2. It's a roulette hopefully you get one, just have to keep retrying to connect and eventually you get one"
      ],
      "metadata": {
        "id": "pm6EDyMQpGOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.distributed.initialize()"
      ],
      "metadata": {
        "id": "0SO3oX1biNNn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUAgOsCgiZiD",
        "outputId": "344ae063-2f2c-4c1f-a782-e29c0bfe2795"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "x = jnp.arange(3 * 2 * 2.).reshape((3, 2, 2))\n",
        "y = jnp.arange(3 * 2 * 2.).reshape((3, 2, 2)) ** 2\n",
        "out = jax.pmap(jnp.dot)(x, y)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-l67CubjBnv",
        "outputId": "d2793334-2e45-44fc-bc63-6b3163a1b26b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[   4.    9.]\n",
            "  [  12.   29.]]\n",
            "\n",
            " [[ 244.  345.]\n",
            "  [ 348.  493.]]\n",
            "\n",
            " [[1412. 1737.]\n",
            "  [1740. 2141.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add(x, y):\n",
        "  return x + y\n",
        "j = jax.jit(add).lower(2,2)\n",
        "k = jax.jit(jnp.dot).lower(2,2)\n",
        "print(k.as_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXhAentzjEz1",
        "outputId": "19705c6b-a941-4e65-8b89-9f824c0afc67"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_dot attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<i32> {mhlo.layout_mode = \"default\"}, %arg1: tensor<i32> {mhlo.layout_mode = \"default\"}) -> (tensor<i32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n",
            "    %0 = stablehlo.convert %arg0 : tensor<i32>\n",
            "    %1 = stablehlo.convert %arg1 : tensor<i32>\n",
            "    %2 = stablehlo.multiply %0, %1 : tensor<i32>\n",
            "    %3 = stablehlo.convert %2 : tensor<i32>\n",
            "    return %3 : tensor<i32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.jit(jax.pmap(jnp.dot)).lower(x,y).as_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkhGVHtijWpO",
        "outputId": "ade67799-f246-4860-97e3-4e915891af58"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_dot attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 3 : i32} {\n",
            "  func.func public @main(%arg0: tensor<3x2x2xf32> {mhlo.layout_mode = \"default\"}, %arg1: tensor<3x2x2xf32> {mhlo.layout_mode = \"default\"}) -> (tensor<3x2x2xf32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n",
            "    %0 = stablehlo.constant dense<0> : tensor<ui32>\n",
            "    %1 = stablehlo.constant dense<1> : tensor<ui32>\n",
            "    %2 = stablehlo.constant dense<3> : tensor<ui32>\n",
            "    %3 = stablehlo.replica_id : tensor<ui32>\n",
            "    %4 = stablehlo.divide %3, %1 : tensor<ui32>\n",
            "    %5 = stablehlo.remainder %4, %2 : tensor<ui32>\n",
            "    %6 = stablehlo.dynamic_slice %arg0, %5, %0, %0, sizes = [1, 2, 2] : (tensor<3x2x2xf32>, tensor<ui32>, tensor<ui32>, tensor<ui32>) -> tensor<1x2x2xf32>\n",
            "    %7 = stablehlo.reshape %6 : (tensor<1x2x2xf32>) -> tensor<2x2xf32>\n",
            "    %8 = stablehlo.constant dense<0> : tensor<ui32>\n",
            "    %9 = stablehlo.constant dense<1> : tensor<ui32>\n",
            "    %10 = stablehlo.constant dense<3> : tensor<ui32>\n",
            "    %11 = stablehlo.replica_id : tensor<ui32>\n",
            "    %12 = stablehlo.divide %11, %9 : tensor<ui32>\n",
            "    %13 = stablehlo.remainder %12, %10 : tensor<ui32>\n",
            "    %14 = stablehlo.dynamic_slice %arg1, %13, %8, %8, sizes = [1, 2, 2] : (tensor<3x2x2xf32>, tensor<ui32>, tensor<ui32>, tensor<ui32>) -> tensor<1x2x2xf32>\n",
            "    %15 = stablehlo.reshape %14 : (tensor<1x2x2xf32>) -> tensor<2x2xf32>\n",
            "    %16 = stablehlo.dot_general %7, %15, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x2xf32>, tensor<2x2xf32>) -> tensor<2x2xf32>\n",
            "    %17 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %18 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<3x2x2xf32>\n",
            "    %19 = stablehlo.constant dense<0> : tensor<ui32>\n",
            "    %20 = stablehlo.constant dense<1> : tensor<ui32>\n",
            "    %21 = stablehlo.constant dense<3> : tensor<ui32>\n",
            "    %22 = stablehlo.replica_id : tensor<ui32>\n",
            "    %23 = stablehlo.divide %22, %20 : tensor<ui32>\n",
            "    %24 = stablehlo.remainder %23, %21 : tensor<ui32>\n",
            "    %25 = stablehlo.broadcast %16, sizes = [1] : (tensor<2x2xf32>) -> tensor<1x2x2xf32>\n",
            "    %26 = stablehlo.dynamic_update_slice %18, %25, %24, %19, %19 : (tensor<3x2x2xf32>, tensor<1x2x2xf32>, tensor<ui32>, tensor<ui32>, tensor<ui32>) -> tensor<3x2x2xf32>\n",
            "    %27 = \"stablehlo.cross-replica-sum\"(%26) {replica_groups = dense<[[0, 1, 2]]> : tensor<1x3xi64>} : (tensor<3x2x2xf32>) -> tensor<3x2x2xf32>\n",
            "    return %27 : tensor<3x2x2xf32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py:1858: UserWarning: The jitted function dot includes a pmap. Using jit-of-pmap can lead to inefficient data movement, as the outer jit does not preserve sharded data representations and instead collects input and output arrays onto a single device. Consider removing the outer jit unless you know what you're doing. See https://github.com/google/jax/issues/2926.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "name": "TPUs in Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}