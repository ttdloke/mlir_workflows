{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Need to have a TPU go to edit and then notebook settings and click TPU V2. It's a roulette hopefully you get one, just have to keep retrying to connect and eventually you get one"
      ],
      "metadata": {
        "id": "pm6EDyMQpGOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.distributed.initialize()"
      ],
      "metadata": {
        "id": "0SO3oX1biNNn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUAgOsCgiZiD",
        "outputId": "921226dc-9aca-4e9c-9443-f74482abe1fe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "x = jnp.arange(164 * 48.).reshape((8, 984))\n",
        "y = jnp.arange(48 * 164.).reshape((8, 984)) ** 2\n",
        "out = jax.pmap(jnp.matmul)(x, y)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-l67CubjBnv",
        "outputId": "3844f108-bb7c-49a0-b695-bb4a6c0b4636"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.3390378e+11 3.5123650e+12 1.5225643e+13 4.0998869e+13 8.6457121e+13\n",
            " 1.5722555e+14 2.5892930e+14 3.9719337e+14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add(x, y):\n",
        "  return x + y\n",
        "j = jax.jit(add).lower(2,2)\n",
        "k = jax.jit(jnp.dot).lower(2,2)\n",
        "print(k.as_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXhAentzjEz1",
        "outputId": "19705c6b-a941-4e65-8b89-9f824c0afc67"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_dot attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<i32> {mhlo.layout_mode = \"default\"}, %arg1: tensor<i32> {mhlo.layout_mode = \"default\"}) -> (tensor<i32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n",
            "    %0 = stablehlo.convert %arg0 : tensor<i32>\n",
            "    %1 = stablehlo.convert %arg1 : tensor<i32>\n",
            "    %2 = stablehlo.multiply %0, %1 : tensor<i32>\n",
            "    %3 = stablehlo.convert %2 : tensor<i32>\n",
            "    return %3 : tensor<i32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.jit(jax.pmap(jnp.matmul)).lower(x,y).as_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkhGVHtijWpO",
        "outputId": "ca3c7f68-5a9c-4b02-f36c-7e5230157ae0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_matmul attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 8 : i32} {\n",
            "  func.func public @main(%arg0: tensor<8x984xf32> {mhlo.layout_mode = \"default\"}, %arg1: tensor<8x984xf32> {mhlo.layout_mode = \"default\"}) -> (tensor<8xf32> {jax.result_info = \"\", mhlo.layout_mode = \"default\"}) {\n",
            "    %0 = stablehlo.constant dense<0> : tensor<ui32>\n",
            "    %1 = stablehlo.constant dense<1> : tensor<ui32>\n",
            "    %2 = stablehlo.constant dense<8> : tensor<ui32>\n",
            "    %3 = stablehlo.replica_id : tensor<ui32>\n",
            "    %4 = stablehlo.divide %3, %1 : tensor<ui32>\n",
            "    %5 = stablehlo.remainder %4, %2 : tensor<ui32>\n",
            "    %6 = stablehlo.dynamic_slice %arg0, %5, %0, sizes = [1, 984] : (tensor<8x984xf32>, tensor<ui32>, tensor<ui32>) -> tensor<1x984xf32>\n",
            "    %7 = stablehlo.reshape %6 : (tensor<1x984xf32>) -> tensor<984xf32>\n",
            "    %8 = stablehlo.constant dense<0> : tensor<ui32>\n",
            "    %9 = stablehlo.constant dense<1> : tensor<ui32>\n",
            "    %10 = stablehlo.constant dense<8> : tensor<ui32>\n",
            "    %11 = stablehlo.replica_id : tensor<ui32>\n",
            "    %12 = stablehlo.divide %11, %9 : tensor<ui32>\n",
            "    %13 = stablehlo.remainder %12, %10 : tensor<ui32>\n",
            "    %14 = stablehlo.dynamic_slice %arg1, %13, %8, sizes = [1, 984] : (tensor<8x984xf32>, tensor<ui32>, tensor<ui32>) -> tensor<1x984xf32>\n",
            "    %15 = stablehlo.reshape %14 : (tensor<1x984xf32>) -> tensor<984xf32>\n",
            "    %16 = stablehlo.dot_general %7, %15, contracting_dims = [0] x [0], precision = [DEFAULT, DEFAULT] : (tensor<984xf32>, tensor<984xf32>) -> tensor<f32>\n",
            "    %17 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n",
            "    %18 = stablehlo.broadcast_in_dim %17, dims = [] : (tensor<f32>) -> tensor<8xf32>\n",
            "    %19 = stablehlo.constant dense<0> : tensor<ui32>\n",
            "    %20 = stablehlo.constant dense<1> : tensor<ui32>\n",
            "    %21 = stablehlo.constant dense<8> : tensor<ui32>\n",
            "    %22 = stablehlo.replica_id : tensor<ui32>\n",
            "    %23 = stablehlo.divide %22, %20 : tensor<ui32>\n",
            "    %24 = stablehlo.remainder %23, %21 : tensor<ui32>\n",
            "    %25 = stablehlo.broadcast %16, sizes = [1] : (tensor<f32>) -> tensor<1xf32>\n",
            "    %26 = stablehlo.dynamic_update_slice %18, %25, %24 : (tensor<8xf32>, tensor<1xf32>, tensor<ui32>) -> tensor<8xf32>\n",
            "    %27 = \"stablehlo.cross-replica-sum\"(%26) {replica_groups = dense<[[0, 1, 2, 3, 4, 5, 6, 7]]> : tensor<1x8xi64>} : (tensor<8xf32>) -> tensor<8xf32>\n",
            "    return %27 : tensor<8xf32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py:1858: UserWarning: The jitted function matmul includes a pmap. Using jit-of-pmap can lead to inefficient data movement, as the outer jit does not preserve sharded data representations and instead collects input and output arrays onto a single device. Consider removing the outer jit unless you know what you're doing. See https://github.com/google/jax/issues/2926.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lO3SEMaIqzP2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "name": "TPUs in Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}